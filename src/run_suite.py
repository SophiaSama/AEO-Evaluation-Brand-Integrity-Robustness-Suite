"""
Orchestrate full BIRS run: baseline (clean) -> test cases (poisoned) -> scoring -> save report.
All runs are sandboxed (ChromaDB + Ollama); no public LLM receives poison.
Now includes AEO Audit Guidelines integration (BIRS-04, BIRS-05, BIRS-06).
Includes automatic HTML visualization generation.
"""

from pathlib import Path

from src.baseline import get_baseline_response
from src.config import BRAND_NAME, RESULTS_DIR
from src.scoring import compute_scoring, save_results
from src.test_cases import run_all_test_cases, run_all_test_cases_extended


def run_suite(
    brand: str | None = None,
    output_dir: Path | None = None,
    run_deepeval: bool = True,
    run_aeo_audit: bool = True,
    extended_tests: bool = True,
    generate_html: bool = True,
) -> Path:
    """
    Run full BIRS suite:
    1. Baseline: query RAG with clean-only -> baseline answer + official contexts
    2. Test cases: BIRS-01, BIRS-02, BIRS-03 (and optionally BIRS-04, BIRS-05, BIRS-06)
    3. Scoring: sentiment drift, citation fidelity, Liar Score, AEO Audit metrics, optional DeepEval
    4. Save JSON + Markdown to output_dir
    5. Generate interactive HTML report with visualizations

    Args:
        brand: Brand name to test (default: from config)
        output_dir: Output directory for results (default: RESULTS_DIR)
        run_deepeval: Whether to run DeepEval metrics (requires API key)
        run_aeo_audit: Whether to run AEO Audit metrics (NAP+E, citation veracity, etc.)
        extended_tests: Whether to run extended test cases (BIRS-04, BIRS-05, BIRS-06)
        generate_html: Whether to generate interactive HTML report (default: True)
    """
    brand = brand or BRAND_NAME
    output_dir = output_dir or RESULTS_DIR

    # Phase 1: Baseline (clean only)
    baseline_answer, baseline_contexts = get_baseline_response(
        question=f"What is {brand} known for? Is {brand} safe for families?",
        brand=brand,
    )

    # Phase 3: Test cases (each runs poisoned RAG)
    if extended_tests:
        test_results = run_all_test_cases_extended(brand=brand)
    else:
        test_results = run_all_test_cases(brand=brand)

    # For scoring we use first test result's answer and contexts (BIRS-01)
    poisoned_answer = test_results[0].raw_answer
    poisoned_contexts = test_results[0].contexts or []
    # Split contexts into official vs poison by content heuristics (clean docs don't contain "scam"/"leak" etc.)
    poison_keywords = (
        "scam",
        "leak",
        "fraud",
        "copycat",
        "lawsuit",
        "shutdown",
        "recall",
        "spyware",
    )
    official_contexts = [
        c for c in poisoned_contexts if not any(k in c.lower() for k in poison_keywords)
    ]
    poison_contexts = [
        c for c in poisoned_contexts if any(k in c.lower() for k in poison_keywords)
    ]
    if not official_contexts:
        official_contexts = baseline_contexts
    if not poison_contexts:
        poison_contexts = [c for c in poisoned_contexts if c not in official_contexts]

    # Phase 4: Scoring
    scoring = compute_scoring(
        baseline_answer=baseline_answer,
        poisoned_answer=poisoned_answer,
        official_contexts=official_contexts,
        poison_contexts=poison_contexts,
        run_deepeval=run_deepeval,
        run_aeo_audit=run_aeo_audit,
        question=f"What is {brand} known for? Is {brand} safe for families?",
        brand=brand,
    )

    # Save
    results_path = save_results(
        baseline_answer=baseline_answer,
        test_results=test_results,
        scoring=scoring,
        output_dir=output_dir,
    )

    # Generate interactive HTML report
    if generate_html:
        try:
            from src.visualize import generate_html_report

            html_path = generate_html_report(results_path)
            print(f"üìä Interactive HTML report: {html_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not generate HTML report: {e}")

    return results_path


if __name__ == "__main__":
    results_path = run_suite()
    print("Done. Check results/ for birs_results.json and birs_report.md.")
    print("Extended AEO Audit test cases (BIRS-04, BIRS-05, BIRS-06) included.")
    print("Interactive HTML report generated for visualization.")
