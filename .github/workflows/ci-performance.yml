name: BIRS Performance Benchmarks

on:
  schedule:
    # Run weekly on Sunday at midnight UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      num_queries:
        description: 'Number of benchmark queries to run'
        required: false
        default: '10'
        type: string
  push:
    branches: [main]
    paths:
      - 'src/rag.py'
      - 'src/baseline.py'
      - 'scripts/ingest_documents.py'

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark-rag:
    name: RAG Query Performance
    runs-on: ubuntu-latest
    
    env:
      PYTHONPATH: ${{ github.workspace }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama and pull model
        run: |
          ollama serve &
          sleep 5
          ollama pull llama3.2
        timeout-minutes: 30

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run ingest and time it
        run: |
          echo "### â±ï¸ Ingestion Performance" >> $GITHUB_STEP_SUMMARY
          
          start_time=$(date +%s)
          python scripts/ingest_documents.py
          end_time=$(date +%s)
          
          duration=$((end_time - start_time))
          echo "- **Ingestion time:** ${duration}s" >> $GITHUB_STEP_SUMMARY

      - name: Benchmark RAG queries
        run: |
          NUM_QUERIES="${{ github.event.inputs.num_queries }}"
          if [ -z "$NUM_QUERIES" ]; then
            NUM_QUERIES="10"
          fi
          export NUM_QUERIES

          python scripts/benchmark_rag.py > benchmark_results.json

      - name: Display benchmark results
        run: |
          echo "### ðŸ“Š RAG Query Benchmarks" >> $GITHUB_STEP_SUMMARY
          python -c "
          import json
          data = json.load(open('benchmark_results.json'))
          
          print(f\"**Queries executed:** {data['num_queries']}\")
          print(f\"**Average latency:** {data['avg_latency_ms']}ms\")
          print(f\"**Min latency:** {data['min_latency_ms']}ms\")
          print(f\"**Max latency:** {data['max_latency_ms']}ms\")
          print(f\"**P95 latency:** {data['p95_latency_ms']}ms\")
          " >> $GITHUB_STEP_SUMMARY

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: rag-benchmark-results
          path: benchmark_results.json

  benchmark-embeddings:
    name: Embedding Performance
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Benchmark embedding generation
        run: python scripts/benchmark_embeddings.py > embedding_benchmark.json

      - name: Display embedding results
        run: |
          echo "### ðŸ”¢ Embedding Performance" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Text Size | Avg Time | Text Length | Embedding Dim |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|----------|-------------|---------------|" >> $GITHUB_STEP_SUMMARY
          python - << 'PYTHON_EOF'
          import json
          data = json.load(open('embedding_benchmark.json'))

          def get_metric(metrics: dict, *keys, default=None):
              for k in keys:
                  if k in metrics:
                      return metrics[k]
              return default

          for size, metrics in data.items():
              avg_time = get_metric(metrics, 'avg_time_ms', 'latency_ms', default='N/A')
              text_len = get_metric(metrics, 'text_length', 'text_len', default='N/A')
              emb_dim = get_metric(metrics, 'embedding_dim', default='N/A')
              suffix = 'ms' if isinstance(avg_time, (int, float)) else ''
              print(f"| {size} | {avg_time}{suffix} | {text_len} chars | {emb_dim} |")
          PYTHON_EOF

      - name: Upload embedding benchmark
        uses: actions/upload-artifact@v4
        with:
          name: embedding-benchmark-results
          path: embedding_benchmark.json

  benchmark-chromadb:
    name: ChromaDB Operations Performance
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Benchmark ChromaDB operations
        run: python scripts/benchmark_chromadb.py > chromadb_benchmark.json

      - name: Display ChromaDB results
        run: |
          echo "### ðŸ’¾ ChromaDB Performance" >> $GITHUB_STEP_SUMMARY
          python -c "
          import json
          data = json.load(open('chromadb_benchmark.json'))
          
          print('**Operation Latencies:**')
          for op, latency in data.items():
              op_name = op.replace('_', ' ').title()
              print(f'- {op_name}: {latency}ms')
          " >> $GITHUB_STEP_SUMMARY

      - name: Upload ChromaDB benchmark
        uses: actions/upload-artifact@v4
        with:
          name: chromadb-benchmark-results
          path: chromadb_benchmark.json

  benchmark-history:
    name: Store Benchmark History
    runs-on: ubuntu-latest
    needs: [benchmark-rag, benchmark-embeddings, benchmark-chromadb]
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          path: benchmarks/

      - name: Commit benchmark history
        run: |
          # Create benchmark history directory
          mkdir -p .benchmark-history
          
          # Copy results with timestamp
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          mkdir -p .benchmark-history/$TIMESTAMP
          cp -r benchmarks/* .benchmark-history/$TIMESTAMP/ || true
          
          # Create summary file
          cat > .benchmark-history/$TIMESTAMP/summary.txt << EOF
          Benchmark Run: $TIMESTAMP
          Commit: ${{ github.sha }}
          Branch: ${{ github.ref_name }}
          Trigger: ${{ github.event_name }}
          EOF
          
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .benchmark-history/$TIMESTAMP/
          git commit -m "ðŸ“Š Add benchmark results for $TIMESTAMP" || echo "No changes to commit"

      - name: Create and push to a new branch
        run: |
          BRANCH_NAME="update-birs-automated-$(date +%Y%m%d%H%M%S)"
          git checkout -b $BRANCH_NAME
          git add .benchmark-history/$TIMESTAMP/
          git commit -m "ðŸ“Š Add benchmark results for $TIMESTAMP" || echo "No changes to commit"
          git push origin $BRANCH_NAME
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v6
        with:
          branch: ${{ steps.create_and_push_branch.outputs.branch }}
          title: "Automated update by CI"
          body: "This PR was created automatically by the BIRS Performance Benchmarks workflow."
          token: ${{ secrets.GITHUB_TOKEN }}
