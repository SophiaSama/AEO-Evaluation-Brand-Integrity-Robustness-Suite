name: BIRS Nightly Extended Tests

on:
  schedule:
    # Run nightly at 1 AM UTC
    - cron: '0 1 * * *'
  workflow_dispatch:
    inputs:
      run_deepeval:
        description: 'Run DeepEval metrics (requires API key)'
        required: false
        default: false
        type: boolean
      run_multi_model:
        description: 'Test with multiple LLM models'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  issues: write

jobs:
  extended-aeo-tests:
    name: Extended AEO Audit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    env:
      PYTHONPATH: ${{ github.workspace }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama and pull model
        run: |
          ollama serve &
          sleep 5
          ollama pull llama3.2
        timeout-minutes: 30

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ðŸ”’ Verify Local-Only Execution
        run: |
          echo "ðŸ”’ Verifying all LLM calls are local-only..."
          
          # Verify Ollama is localhost
          curl -f http://localhost:11434/api/tags > /dev/null || (echo "ERROR: Ollama not on localhost" && exit 1)
          
          # Verify no cloud keys
          [ -z "$OPENAI_API_KEY" ] || (echo "WARNING: OPENAI_API_KEY is set" && exit 1)
          [ -z "$ANTHROPIC_API_KEY" ] || (echo "WARNING: ANTHROPIC_API_KEY is set" && exit 1)
          
          # Verify config
          python -c "from src.config import OLLAMA_BASE_URL; assert 'localhost' in OLLAMA_BASE_URL, f'Not localhost: {OLLAMA_BASE_URL}'"
          
          echo "âœ“ Security verified: All LLM inference is local-only"

      - name: Run ingest
        run: python scripts/ingest_documents.py
        timeout-minutes: 10

      - name: Run full BIRS suite with extended tests
        run: |
          python << 'PYTHON_SCRIPT' > extended_results.json
import json
from src.run_suite import run_suite

# Run with all extended tests enabled
results_path = run_suite(
    extended_tests=True,
    run_aeo_audit=True,
    run_deepeval=False  # Don't require API key for nightly
)

print(f'Results saved to: {results_path}')

# Load and summarize results
with open(results_path) as f:
    data = json.load(f)

summary = {
    'total_tests': len([k for k in data.keys() if k.startswith('birs_')]),
    'baseline_answer_length': len(data.get('baseline_answer', '')),
    'contexts_count': len(data.get('contexts', [])),
    'extended_tests_completed': True
}

# Check test results
test_results = {}
for key in data.keys():
    if key.startswith('birs_'):
        test_results[key] = {
            'pass': data[key].get('pass', False),
            'score': data[key].get('score', 0)
        }

summary['test_results'] = test_results
summary['passing_tests'] = sum(1 for t in test_results.values() if t['pass'])
summary['avg_score'] = sum(t['score'] for t in test_results.values()) / len(test_results) if test_results else 0

with open('extended_results.json', 'w') as f:
    json.dump(summary, f, indent=2)

print(json.dumps(summary, indent=2))
PYTHON_SCRIPT
        timeout-minutes: 30

      - name: Display extended test results
        run: |
          echo "### ðŸ§ª Extended AEO Test Results" >> $GITHUB_STEP_SUMMARY
          python -c "
          import json
          data = json.load(open('extended_results.json'))
          
          print(f\"**Total tests:** {data['total_tests']}\")
          print(f\"**Passing tests:** {data['passing_tests']}\")
          print(f\"**Average score:** {data['avg_score']:.2f}\")
          print('')
          print('**Test Details:**')
          for test_name, result in data['test_results'].items():
              status = 'âœ…' if result['pass'] else 'âŒ'
              print(f\"- {status} {test_name}: {result['score']:.2f}\")
          " >> $GITHUB_STEP_SUMMARY

      - name: Check for regressions
        run: |
          python << 'EOF'
import json

data = json.load(open('extended_results.json'))

# Define minimum acceptable scores
MIN_ACCEPTABLE_SCORES = {
    "birs_01": 0.5,  # Grounding
    "birs_02": 0.5,  # Consistency
    "birs_03": 0.5,  # Robustness
    "birs_04": 0.6,  # Entity integrity
    "birs_05": 0.6,  # Citation veracity
    "birs_06": 0.6,  # Source attribution
}

regressions = []
for test_name, result in data['test_results'].items():
    min_score = MIN_ACCEPTABLE_SCORES.get(test_name, 0.5)
    if result['score'] < min_score:
        regressions.append(f"{test_name}: {result['score']:.2f} < {min_score}")

if regressions:
    print("âš ï¸ Performance regressions detected:")
    for reg in regressions:
        print(f"  - {reg}")
    
    # Create issue if regressions found
    print("\n::warning::Performance regressions detected in nightly tests")
else:
    print("âœ“ No regressions detected")
EOF

      - name: Upload extended test results
        uses: actions/upload-artifact@v4
        with:
          name: extended-test-results
          path: |
            extended_results.json
            results/birs_results.json

  multi-model-comparison:
    name: Multi-Model Comparison
    runs-on: ubuntu-latest
    if: github.event.inputs.run_multi_model == 'true'
    timeout-minutes: 90
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama
        run: |
          ollama serve &
          sleep 5

      - name: Pull multiple models
        run: |
          echo "Pulling models for comparison..."
          ollama pull llama3.2
          ollama pull mistral
          ollama pull phi3
        timeout-minutes: 60

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run ingest
        run: python scripts/ingest_documents.py

      - name: Test with multiple models
        run: |
          python << 'EOF' > multi_model_results.json
import json
import os
from src.run_suite import run_suite

models = ["llama3.2", "mistral", "phi3"]
all_results = {}

for model in models:
    print(f"\n{'='*60}")
    print(f"Testing with model: {model}")
    print('='*60)
    
    # Set environment variable for model selection
    os.environ['OLLAMA_MODEL'] = model
    
    try:
        results_path = run_suite(
            extended_tests=True,
            run_aeo_audit=True,
            run_deepeval=False
        )
        
        with open(results_path) as f:
            data = json.load(f)
        
        # Extract key metrics
        model_results = {
            "baseline_answer": data.get("baseline_answer", ""),
            "tests": {}
        }
        
        for key in data.keys():
            if key.startswith("birs_"):
                model_results["tests"][key] = {
                    "pass": data[key].get("pass", False),
                    "score": data[key].get("score", 0)
                }
        
        all_results[model] = model_results
        
    except Exception as e:
        print(f"Error testing {model}: {e}")
        all_results[model] = {"error": str(e)}

# Save comparison
with open("multi_model_results.json", "w") as f:
    json.dump(all_results, f, indent=2)

print("\n" + "="*60)
print("Multi-Model Comparison Complete")
print("="*60)
EOF

      - name: Display model comparison
        run: |
          echo "### ðŸ¤– Multi-Model Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          python -c "
          import json
          
          data = json.load(open('multi_model_results.json'))
          
          # Create comparison table
          print('| Model | Avg Score | Passing Tests |')
          print('|-------|-----------|---------------|')
          
          for model, results in data.items():
              if 'error' in results:
                  print(f'| {model} | ERROR | - |')
                  continue
              
              tests = results.get('tests', {})
              if tests:
                  avg_score = sum(t['score'] for t in tests.values()) / len(tests)
                  passing = sum(1 for t in tests.values() if t['pass'])
                  print(f'| {model} | {avg_score:.2f} | {passing}/{len(tests)} |')
          " >> $GITHUB_STEP_SUMMARY

      - name: Upload model comparison
        uses: actions/upload-artifact@v4
        with:
          name: multi-model-comparison
          path: multi_model_results.json

  deepeval-metrics:
    name: DeepEval Metrics
    runs-on: ubuntu-latest
    if: github.event.inputs.run_deepeval == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama and pull model
        run: |
          ollama serve &
          sleep 5
          ollama pull llama3.2
        timeout-minutes: 30

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run ingest
        run: python scripts/ingest_documents.py

      - name: Run BIRS with DeepEval
        env:
          DEEPEVAL_API_KEY: ${{ secrets.DEEPEVAL_API_KEY }}
        run: |
          python -c "
          from src.run_suite import run_suite
          
          results_path = run_suite(
              extended_tests=True,
              run_aeo_audit=True,
              run_deepeval=True
          )
          
          print(f'Results with DeepEval: {results_path}')
          "
        timeout-minutes: 30

      - name: Upload DeepEval results
        uses: actions/upload-artifact@v4
        with:
          name: deepeval-results
          path: results/birs_results.json

  create-issue-on-failure:
    name: Create Issue on Test Failure
    runs-on: ubuntu-latest
    needs: [extended-aeo-tests, multi-model-comparison]
    if: failure() && github.event_name == 'schedule'
    
    steps:
      - name: Create issue
        uses: actions/github-script@v7
        with:
          script: |
            const title = `ðŸ”´ Nightly Extended Tests Failed - ${new Date().toISOString().split('T')[0]}`;
            const body = `
            ## Nightly Extended Test Failure
            
            The nightly extended test run has failed.
            
            **Workflow Run:** [${context.runNumber}](${context.payload.repository.html_url}/actions/runs/${context.runId})
            **Triggered:** ${new Date().toISOString()}
            
            ### Failed Jobs
            - Extended AEO Tests: ${{ needs.extended-aeo-tests.result }}
            - Multi-Model Comparison: ${{ needs.multi-model-comparison.result }}
            
            ### Action Required
            1. Review the workflow logs
            2. Check for performance regressions
            3. Investigate any model-specific issues
            4. Update tests or fix code as needed
            
            ---
            *This issue was automatically created by the nightly extended tests workflow.*
            `;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['automated-test', 'nightly-failure', 'needs-investigation']
            });
