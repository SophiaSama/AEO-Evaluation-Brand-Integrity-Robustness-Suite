name: BIRS Nightly Extended Tests

on:
  schedule:
    # Run nightly at 1 AM UTC
    - cron: '0 1 * * *'
  workflow_dispatch:
    inputs:
      run_deepeval:
        description: 'Run DeepEval metrics (requires API key)'
        required: false
        default: false
        type: boolean
      run_multi_model:
        description: 'Test with multiple LLM models'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  issues: write

jobs:
  extended-aeo-tests:
    name: Extended AEO Audit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    env:
      PYTHONPATH: ${{ github.workspace }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama and pull model
        run: |
          ollama serve &
          sleep 5
          ollama pull llama3.2
        timeout-minutes: 30

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ðŸ”’ Verify Local-Only Execution
        run: |
          echo "ðŸ”’ Verifying all LLM calls are local-only..."
          
          # Verify Ollama is localhost
          curl -f http://localhost:11434/api/tags > /dev/null || (echo "ERROR: Ollama not on localhost" && exit 1)
          
          # Verify no cloud keys
          [ -z "$OPENAI_API_KEY" ] || (echo "WARNING: OPENAI_API_KEY is set" && exit 1)
          [ -z "$ANTHROPIC_API_KEY" ] || (echo "WARNING: ANTHROPIC_API_KEY is set" && exit 1)
          
          # Verify config
          python -c "from src.config import OLLAMA_BASE_URL; assert 'localhost' in OLLAMA_BASE_URL, f'Not localhost: {OLLAMA_BASE_URL}'"
          
          echo "âœ“ Security verified: All LLM inference is local-only"

      - name: Run ingest
        run: python scripts/ingest_documents.py
        timeout-minutes: 10

      - name: Run full BIRS suite with extended tests
        run: |
          python scripts/nightly_extended_results.py > extended_results.json
        timeout-minutes: 30

      - name: Display extended test results
        run: |
          echo "### ðŸ§ª Extended AEO Test Results" >> $GITHUB_STEP_SUMMARY
          python -c "
          import json
          data = json.load(open('extended_results.json'))
          
          print(f\"**Total tests:** {data['total_tests']}\")
          print(f\"**Passing tests:** {data['passing_tests']}\")
          print(f\"**Average score:** {data['avg_score']:.2f}\")
          print('')
          print('**Test Details:**')
          for test_name, result in data['test_results'].items():
              status = 'âœ…' if result['pass'] else 'âŒ'
              print(f\"- {status} {test_name}: {result['score']:.2f}\")
          " >> $GITHUB_STEP_SUMMARY

      - name: Check for regressions
        run: python scripts/nightly_check_regressions.py

      - name: Upload extended test results
        uses: actions/upload-artifact@v4
        with:
          name: extended-test-results
          path: |
            extended_results.json
            results/birs_results.json

  multi-model-comparison:
    name: Multi-Model Comparison
    runs-on: ubuntu-latest
    if: github.event.inputs.run_multi_model == 'true'
    timeout-minutes: 90
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama
        run: |
          ollama serve &
          sleep 5

      - name: Pull multiple models
        run: |
          echo "Pulling models for comparison..."
          ollama pull llama3.2
          ollama pull mistral
          ollama pull phi3
        timeout-minutes: 60

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run ingest
        run: python scripts/ingest_documents.py

      - name: Run multi-model tests
        if: ${{ github.event.inputs.run_multi_model || false }}
        run: python scripts/nightly_multi_model.py > multi_model_results.json
        timeout-minutes: 30

      - name: Display model comparison
        run: |
          echo "### ðŸ¤– Multi-Model Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          python -c "
          import json
          
          data = json.load(open('multi_model_results.json'))
          
          # Create comparison table
          print('| Model | Avg Score | Passing Tests |')
          print('|-------|-----------|---------------|')
          
          for model, results in data.items():
              if 'error' in results:
                  print(f'| {model} | ERROR | - |')
                  continue
              
              tests = results.get('tests', {})
              if tests:
                  avg_score = sum(t['score'] for t in tests.values()) / len(tests)
                  passing = sum(1 for t in tests.values() if t['pass'])
                  print(f'| {model} | {avg_score:.2f} | {passing}/{len(tests)} |')
          " >> $GITHUB_STEP_SUMMARY

      - name: Upload model comparison
        uses: actions/upload-artifact@v4
        with:
          name: multi-model-comparison
          path: multi_model_results.json

  deepeval-metrics:
    name: DeepEval Metrics
    runs-on: ubuntu-latest
    if: github.event.inputs.run_deepeval == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama and pull model
        run: |
          ollama serve &
          sleep 5
          ollama pull llama3.2
        timeout-minutes: 30

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run ingest
        run: python scripts/ingest_documents.py

      - name: Run BIRS with DeepEval
        env:
          DEEPEVAL_API_KEY: ${{ secrets.DEEPEVAL_API_KEY }}
        run: |
          python -c "
          from src.run_suite import run_suite
          
          results_path = run_suite(
              extended_tests=True,
              run_aeo_audit=True,
              run_deepeval=True
          )
          
          print(f'Results with DeepEval: {results_path}')
          "
        timeout-minutes: 30

      - name: Upload DeepEval results
        uses: actions/upload-artifact@v4
        with:
          name: deepeval-results
          path: results/birs_results.json

  create-issue-on-failure:
    name: Create Issue on Test Failure
    runs-on: ubuntu-latest
    needs: [extended-aeo-tests, multi-model-comparison]
    if: failure() && github.event_name == 'schedule'
    
    steps:
      - name: Create issue
        uses: actions/github-script@v7
        with:
          script: |
            const title = `ðŸ”´ Nightly Extended Tests Failed - ${new Date().toISOString().split('T')[0]}`;
            const body = `
            ## Nightly Extended Test Failure
            
            The nightly extended test run has failed.
            
            **Workflow Run:** [${context.runNumber}](${context.payload.repository.html_url}/actions/runs/${context.runId})
            **Triggered:** ${new Date().toISOString()}
            
            ### Failed Jobs
            - Extended AEO Tests: ${{ needs.extended-aeo-tests.result }}
            - Multi-Model Comparison: ${{ needs.multi-model-comparison.result }}
            
            ### Action Required
            1. Review the workflow logs
            2. Check for performance regressions
            3. Investigate any model-specific issues
            4. Update tests or fix code as needed
            
            ---
            *This issue was automatically created by the nightly extended tests workflow.*
            `;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['automated-test', 'nightly-failure', 'needs-investigation']
            });
